{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33adbdc4",
   "metadata": {},
   "source": [
    "# Model Evaluation: Evaluation Metrics and Cross-Validation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e863481f",
   "metadata": {},
   "source": [
    "- After building a machine learning model, it's crucial to assess how well the model performs on unseen data. \n",
    "\n",
    "- Model evaluation helps ensure that the model generalizes well beyond the training set. \n",
    "\n",
    "- This step involves using various evaluation metrics and cross-validation methods to test the model’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a20c409",
   "metadata": {},
   "source": [
    "## 1. Evaluation Metrics\n",
    "Different types of machine learning tasks (classification, regression, etc.) require different evaluation metrics. \n",
    "- Below are some common metrics used for both classification and regression models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd3e7f5",
   "metadata": {},
   "source": [
    "### Classification Metrics\n",
    "1. Accuracy\n",
    "\n",
    "- Definition: The ratio of correctly predicted instances to the total instances.\n",
    "- Use Case: Best for balanced datasets, where the classes are equally represented.\n",
    "- Formula: Accuracy = Correct Predictions/Total Predictions\n",
    "\n",
    "           Accuracy = 10[Correct Predictions]/20[Total Correct Predictions] =? 50%\n",
    "\n",
    "- Example: If your model correctly predicts 90 out of 100 images as either cats or dogs, the accuracy is 90%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed91f143",
   "metadata": {},
   "source": [
    "Loan Prediction System --> If loan to be granted or not granted\n",
    "Records/Instances/Rows - 1000 [Male - 800[granted/not granted], Female - 200[granted/not granted]]--> Unbalanced/Biased Dataset\n",
    "Records/Instances/Rows - 1000 [Male - 550, Female - 450]--> Balanced/Unbiased Dataset\n",
    "\n",
    "Creating ML model -> training it on above dataset --> Predicting --> ?Biased\n",
    "\n",
    "Ok - 1+\n",
    "Not Ok - 1+1+1+1+1+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6926da1",
   "metadata": {},
   "source": [
    "2. Precision, Recall, and F1-Score\n",
    "\n",
    "Precision: Measures how many of the predicted positive instances are actually positive.\n",
    "\n",
    "    Precision = True Positives/True Positives+False Positives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08686fe",
   "metadata": {},
   "source": [
    "Emails - Spam Detection\n",
    "Let’s take the example of spam detection in emails:\n",
    "\n",
    "- True Positive (TP): The model correctly identifies an email as spam (the email is actually spam and the model predicted it as spam).\n",
    "\n",
    "Example: A marketing email gets correctly flagged as spam.\n",
    "\n",
    "- False Positive (FP): The model incorrectly identifies a non-spam email as spam (the email is not spam, but the model flagged it as spam).\n",
    "\n",
    "Example: An important work email gets flagged as spam incorrectly.\n",
    "\n",
    "- True Positive (TP): Actual positive, predicted positive.\n",
    "- False Positive (FP): Actual negative, predicted positive.\n",
    "- False Negative (FN): Actual positive, predicted negative.\n",
    "- True Negative (TN): Actual negative, predicted negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8564724b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Labels (y_true): [0 1 0 0 0 1 0 0 0 1]\n",
      "Predictions (y_pred): [0 0 0 0 1 0 1 1 1 0]\n",
      "True Positives (TP): 0\n",
      "False Positives (FP): 4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create synthetic true labels (y_true) and predictions (y_pred)\n",
    "np.random.seed(42)  # For reproducibility\n",
    "y_true = np.random.randint(0, 2, size=10)  # True labels (binary: 0 or 1)\n",
    "y_pred = np.random.randint(0, 2, size=10)  # Model predictions (binary: 0 or 1)\n",
    "\n",
    "print(\"True Labels (y_true):\", y_true)\n",
    "print(\"Predictions (y_pred):\", y_pred)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "# Output the results\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de3995",
   "metadata": {},
   "source": [
    "### Recall (Sensitivity): \n",
    "- Measures how many actual positives are correctly predicted.\n",
    "\n",
    "    Recall= True Positives/True Positives+False Negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e47c824",
   "metadata": {},
   "source": [
    "### F1-Score: \n",
    "- Harmonic mean of precision and recall. Best for imbalanced datasets.\n",
    "\n",
    "    F1 = 2(Precision*Recall/Precision + Recall)\n",
    "    \n",
    "    <br>\n",
    "    \n",
    "- Example: In medical diagnosis, if the model predicts the presence of a disease, precision would tell you how often the model is correct when it says the disease is present, and recall tells how often it detects all cases of the disease.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f5e856",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "Definition: A table used to describe the performance of a classification model by displaying true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "- Example: For binary classification, it helps identify how well the model differentiates between two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687e1bd0",
   "metadata": {},
   "source": [
    "### ROC-AUC (Receiver Operating Characteristic - Area Under the Curve)\n",
    "\n",
    "- Definition: Measures the performance of a classification model at different threshold levels. AUC evaluates the trade-off between true positive rate and false positive rate.\n",
    "\n",
    "\n",
    "- Use Case: Used when you want to measure the probability estimates of classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256fec54",
   "metadata": {},
   "source": [
    "## Regression Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e16335a",
   "metadata": {},
   "source": [
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "- Definition: The average of the absolute differences between the predicted and actual values.\n",
    "\n",
    "- Use Case: Provides a clear idea of how far predictions deviate from actual values.\n",
    "\n",
    "- Example: If the model predicts house prices, MAE will tell the average dollar amount by which the predicted price is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5853f11",
   "metadata": {},
   "source": [
    "### Mean Squared Error (MSE)\n",
    "\n",
    "- Definition: The average of the squared differences between the predicted and actual values.\n",
    "\n",
    "- Use Case: Punishes larger errors more heavily than MAE.\n",
    "\n",
    "- Example: Useful in regression problems like stock price predictions, where large errors can have bigger consequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b545b39",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error (RMSE)\n",
    "\n",
    "- Definition: The square root of MSE, giving the error in the same units as the target variable.\n",
    "\n",
    "- Use Case: A more interpretable version of MSE.\n",
    "\n",
    "- Example: If predicting energy consumption, RMSE tells how off the predictions are in kilowatt-hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dddc3b",
   "metadata": {},
   "source": [
    "### R-Squared (R²)\n",
    "\n",
    "- Definition: Measures the proportion of the variance in the dependent variable that is predictable from the independent variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea020584",
   "metadata": {},
   "source": [
    "# Cross-Validation Methods\n",
    "Cross-validation helps to estimate the performance of a model on unseen data. It divides the data into subsets to ensure that the model is not overfitting or underfitting.\n",
    "\n",
    "- Holdout Method\n",
    "    - Definition: The dataset is split into two sets: a training set and a test set.\n",
    "    - Drawback: The evaluation is dependent on a single split, which can lead to high variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ce473",
   "metadata": {},
   "source": [
    "### K-Fold Cross-Validation\n",
    "- Definition: The data is split into K equal-sized subsets, or folds. The model is trained K times, each time using a different fold as the test set and the remaining folds as the training set.\n",
    "- How it Works:\n",
    "    - Split the dataset into K parts (e.g., 5 or 10).\n",
    "    - Train the model on K-1 parts, then test it on the remaining part.\n",
    "    - Repeat the process K times, each time using a different fold as the test set.\n",
    "    - Calculate the average of the evaluation metric across all K iterations.\n",
    "- Example: For customer segmentation, the dataset is split into 5 folds, ensuring that the model generalizes well across all customer groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2726238a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a635533",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test on 1 --> train on others or vise-a-versa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
