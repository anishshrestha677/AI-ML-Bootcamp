{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96f3342c",
   "metadata": {},
   "source": [
    "### Problem Statement:\n",
    "Suppose we are building a sentiment analysis system for movie reviews to determine whether reviews are positive or negative. The reviews contain a mix of casual language, punctuations, numbers, special characters, and stop words that do not contribute to sentiment analysis. The goal is to preprocess the text data efficiently to improve the performance of the sentiment classifier.\n",
    "\n",
    "For this, we need to implement several key text preprocessing steps, such as tokenization, lowercasing, removing stop words, punctuation removal, stemming, lemmatization, handling numbers, removing special characters, and text normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93479d60",
   "metadata": {},
   "source": [
    "### 1. Tokenization\n",
    "Tokenization is the process of splitting a piece of text into individual words (tokens) or sentences. For this problem, we will tokenize each review into words for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7adc00e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'movie', 'was', 'AMAZING', '!', '!', 'I', 'loved', 'it', '!', '10/10'], ['Worst', 'movie', 'ever', '.', 'Total', 'waste', 'of', 'time', '.', '0/10']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/devarsh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')  # Download tokenization models\n",
    "\n",
    "reviews = [\"The movie was AMAZING!! I loved it! 10/10\", \n",
    "           \"Worst movie ever. Total waste of time. 0/10\"]\n",
    "           \n",
    "# Tokenizing the reviews\n",
    "tokenized_reviews = [word_tokenize(review) for review in reviews]\n",
    "print(tokenized_reviews)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25719190",
   "metadata": {},
   "source": [
    "### 2. Lowercasing\n",
    "Lowercasing ensures that words like “Movie” and “movie” are treated the same by converting everything to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63d85e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'movie', 'was', 'amazing', '!', '!', 'i', 'loved', 'it', '!', '10/10'], ['worst', 'movie', 'ever', '.', 'total', 'waste', 'of', 'time', '.', '0/10']]\n"
     ]
    }
   ],
   "source": [
    "# Converting tokens to lowercase\n",
    "lowercased_reviews = [[word.lower() for word in tokens] for tokens in tokenized_reviews]\n",
    "print(lowercased_reviews)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2b72f6",
   "metadata": {},
   "source": [
    "### 3. Removing Stop Words\n",
    "Stop words (e.g., \"the\", \"is\", \"in\") are common words that don’t add significant meaning in text classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59ad63a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['movie', 'amazing', '!', '!', 'loved', '!', '10/10'], ['worst', 'movie', 'ever', '.', 'total', 'waste', 'time', '.', '0/10']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/devarsh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Defining the set of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Removing stop words\n",
    "filtered_reviews = [[word for word in tokens if word not in stop_words] for tokens in lowercased_reviews]\n",
    "print(filtered_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba533f92",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80813c03",
   "metadata": {},
   "source": [
    "### 4. Removing Punctuation\n",
    "Punctuation can often be noise in text processing tasks, so we will remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e7e6319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['movie', 'amazing', 'loved', '10/10'], ['worst', 'movie', 'ever', 'total', 'waste', 'time', '0/10']]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Removing punctuation\n",
    "reviews_no_punct = [[word for word in tokens if word not in string.punctuation] for tokens in filtered_reviews]\n",
    "print(reviews_no_punct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52808d8",
   "metadata": {},
   "source": [
    "### 5. Stemming and Lemmatization\n",
    "Stemming reduces words to their base form (e.g., \"loved\" to \"love\"), while lemmatization reduces words to their lemma (e.g., \"better\" to \"good\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bba46116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['movi', 'amaz', 'love', '10/10'], ['worst', 'movi', 'ever', 'total', 'wast', 'time', '0/10']]\n",
      "[['movie', 'amazing', 'loved', '10/10'], ['worst', 'movie', 'ever', 'total', 'waste', 'time', '0/10']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/devarsh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Applying stemming and lemmatization\n",
    "stemmed_reviews = [[stemmer.stem(word) for word in tokens] for tokens in reviews_no_punct]\n",
    "lemmatized_reviews = [[lemmatizer.lemmatize(word) for word in tokens] for tokens in reviews_no_punct]\n",
    "print(stemmed_reviews)\n",
    "print(lemmatized_reviews)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30de2ad5",
   "metadata": {},
   "source": [
    "### 6. Handling Numbers\n",
    "Numbers may or may not be important in some tasks, but for sentiment analysis, ratings like \"10/10\" or \"0/10\" can be valuable. We will remove other numbers and retain sentiment-related ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87862831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['movie', 'amazing', 'loved', '10/10'], ['worst', 'movie', 'ever', 'total', 'waste', 'time', '0/10']]\n"
     ]
    }
   ],
   "source": [
    "# Removing standalone numbers, while retaining sentiment ratings (e.g., \"10/10\")\n",
    "reviews_no_numbers = [[word for word in tokens if not word.isdigit() or word in ['10', '0']] for tokens in lemmatized_reviews]\n",
    "print(reviews_no_numbers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b240c94",
   "metadata": {},
   "source": [
    "### 7. Removing Special Characters\n",
    "Special characters such as “#”, “@”, or emoji may need to be removed or treated based on their relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93c3dbe8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['movie', 'amazing', 'loved', '1010'], ['worst', 'movie', 'ever', 'total', 'waste', 'time', '010']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Removing special characters\n",
    "def remove_special_chars(tokens):\n",
    "    return [re.sub(r'[^A-Za-z0-9]+', '', word) for word in tokens]\n",
    "\n",
    "reviews_cleaned = [remove_special_chars(tokens) for tokens in reviews_no_numbers]\n",
    "print(reviews_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221783c7",
   "metadata": {},
   "source": [
    "### 8. Text Normalization\n",
    "Text normalization ensures consistency across words by transforming them into a standard format. This includes correcting spelling mistakes, expanding contractions, and dealing with abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de16e1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['movie', 'amazing', 'loved', '1010'], ['worst', 'movie', 'ever', 'total', 'waste', 'time', '010']]\n"
     ]
    }
   ],
   "source": [
    "# Expanding common contractions (e.g., \"I've\" to \"I have\")\n",
    "contractions = {\"i've\": \"i have\", \"it's\": \"it is\", \"won't\": \"will not\", \"can't\": \"cannot\",\"4\":\"for\"}\n",
    "#contractions\n",
    "\n",
    "def expand_contractions(tokens):\n",
    "    return [contractions[word] if word in contractions else word for word in tokens]\n",
    "\n",
    "normalized_reviews = [expand_contractions(tokens) for tokens in reviews_cleaned]\n",
    "print(normalized_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cf4e06",
   "metadata": {},
   "source": [
    "### Amazon Sentiment Sleuth : Analyzing Product Reviews with LLMs\n",
    "Build and evaluate an NLP model for sentiment analysis of product reviews using the Amazon Customer Reviews dataset from AWS. Students will apply text preprocessing, classification techniques, and advanced NLP methods to analyze sentiment in reviews.\n",
    "\n",
    "\n",
    "Project Deliverables.\n",
    "\u0004 Model cod5\n",
    "\u0004 Sentiment analysis result\u001f\n",
    "\u0004 Jupyter notebook with documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecad1744",
   "metadata": {},
   "source": [
    "### NLP with Python: Using NLTK and SpaCy for text classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94c6d93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0213767a",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'reviews'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load English tokenizer, POS tagger, parser, NER, and word vectors\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreviews\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Function to preprocess text using SpaCy\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text_spacy\u001b[39m(text):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m     52\u001b[0m         name,\n\u001b[1;32m     53\u001b[0m         vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[1;32m     54\u001b[0m         disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[1;32m     55\u001b[0m         enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[1;32m     56\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[1;32m     57\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     58\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/spacy/util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'reviews'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# Load English tokenizer, POS tagger, parser, NER, and word vectors\n",
    "nlp = spacy.load('')\n",
    "\n",
    "# Function to preprocess text using SpaCy\n",
    "def preprocess_text_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Tokenization, Lowercasing, Removing stop words and punctuation, and Lemmatization\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Preprocess reviews\n",
    "for review in reviews:\n",
    "    cleaned_tokens = preprocess_text_spacy(review)\n",
    "    print(f\"Original: {review}\")\n",
    "    print(f\"Cleaned Tokens: {cleaned_tokens}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1deeac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
